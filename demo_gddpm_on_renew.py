# -*- coding: utf-8 -*-
"""Demo GDDPM on renew.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-lEhSCjIVuksg7XwkIJ6TzYYEHvTAVhQ
"""

!git clone https://github.com/AmirMiraki/GDDPM.git

!cp -R /content/GDDPM/Algorithm ./Algorithm

!pip install torch==2.3.0
!pip install CRPS
import torch
print(torch.__version__,torch.version.cuda)

!python -m pip install pip==23
!pip install pytorchts lightning
!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html
!pip install gluonts==0.9.9

# Commented out IPython magic to ensure Python compatibility.
import importlib
# %matplotlib inline
import matplotlib.pyplot as plt

import numpy as np
import pandas as pd
import torch
from gluonts.dataset.multivariate_grouper import MultivariateGrouper
from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset
from gluonts.evaluation.backtest import make_evaluation_predictions
from gluonts.evaluation import MultivariateEvaluator
from Algorithm import GDDPMEstimator
from pts.model.transformer_tempflow import TransformerTempFlowEstimator
from pts import Trainer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Available datasets: {list(dataset_recipes.keys())}")

import pandas as pd
X_f = pd.read_csv("/content/GDDPM/Data/Renew.csv",index_col='Time',parse_dates=True).resample('1h').mean()
time_length = X_f.shape[0]
num_nodes = X_f.shape[1]-3
test_length = round(0.2 * time_length)
val_length = round(0.2 * time_length)
train_length = time_length - val_length - test_length
start = pd.Period(X_f.index[0],freq="10min")
X = X_f.to_numpy().T
A = np.ones((num_nodes,num_nodes))
edge_index = np.nonzero(A)
edge_weight = np.float32(A[edge_index])
from gluonts.dataset.common import ListDataset
train_ds = ListDataset([{"target": x, "start": X_f.index[0]} for x in X[3:, :-(val_length + test_length)]],freq="1h")
val_ds = ListDataset([{"target": x, "start": X_f.index[-(val_length + test_length)]} for x in X[3:, -(val_length + test_length): -test_length]],freq="1h")
test_ds = ListDataset([{"target": x, "start": X_f.index[-test_length]} for x in X[3:, -( test_length):]],freq="1h")

X_f.head()

train_grouper = MultivariateGrouper(max_target_dim=num_nodes )
val_grouper = MultivariateGrouper(max_target_dim=num_nodes )
test_grouper = MultivariateGrouper(max_target_dim=num_nodes )
dataset_train = train_grouper(train_ds)
dataset_val = val_grouper(val_ds)
dataset_test = test_grouper(test_ds)

H=24
M=24
estimator = GDDPMEstimator(
    target_dim=num_nodes,
    prediction_length=H,
    context_length=M,
    cell_type='LSTM',
    input_size=40,
    freq="1h",
    loss_type='l2',
    scaling=True,
    diff_steps=100,
    beta_end=0.1,
    beta_schedule="linear",
    edge_index=edge_index,
    edge_weight=None,
    trainer=Trainer(device=device,
                    epochs=20,
                    learning_rate=1e-3,
                    num_batches_per_epoch=100,
                    batch_size=64,)
)

predictor = estimator.train(dataset_train, num_workers=0,prefetch_factor = None)

RMSE = []
MAE  = []
MAPE = []
for i in range(1):
  forecast_it, ts_it = make_evaluation_predictions(dataset=dataset_val,predictor=predictor,num_samples=100)
  forecasts = list(forecast_it)
  targets = list(ts_it)

  for tts, forecast in zip(targets, forecasts):
      true_target = tts.values[-H:,:]
      RMSE.append(np.sqrt(((true_target - forecast.mean[:,:])**2).mean()) )
      MAE.append((np.abs(true_target - forecast.mean[:,:])).mean())
      MAPE.append((np.abs(true_target - forecast.mean[:,:]) / np.abs(true_target)).mean() )

print('RMSE: {:}'.format(np.mean(RMSE)))
print('MAE: {:}'.format(np.mean(MAE)))
print('MAPE: {:}'.format(np.mean(MAPE)))

evaluator = MultivariateEvaluator(quantiles=(np.arange(20)/20.0)[1:],
                                  target_agg_funcs={'sum': np.sum})
agg_metric, item_metrics = evaluator(targets, forecasts, num_series=len(dataset_test))
print("CRPS:", agg_metric["mean_wQuantileLoss"])
print("ND:", agg_metric["ND"])
print("NRMSE:", agg_metric["NRMSE"])
print("")
print("CRPS-Sum:", agg_metric["m_sum_mean_wQuantileLoss"])
print("ND-Sum:", agg_metric["m_sum_ND"])
print("NRMSE-Sum:", agg_metric["m_sum_NRMSE"])

def plot(target, forecast, prediction_length,contex_length, prediction_intervals=(50.0, 90.0), color='g', fname=None):
    label_prefix = ""
    rows = 9
    cols = 1
    fig, axs = plt.subplots(rows, cols, figsize=(24, 24),constrained_layout=True)
    fig.tight_layout()
    axx = axs.ravel()
    seq_len, target_dim = target.shape

    ps = [50.0] + [
            50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, +1.0]
        ]

    percentiles_sorted = sorted(set(ps))

    def alpha_for_percentile(p):
        return (p / 100.0) ** 0.3

    for dim in range(0, min(rows * cols, target_dim)):
        ax = axx[dim]

        target[-(contex_length+ prediction_length) :][dim].plot(ax=ax, ls="-",marker="o")

        ps_data = [forecast.quantile(p / 100.0)[:,dim] for p in percentiles_sorted]
        i_p50 = len(percentiles_sorted) // 2
        p50_data = ps_data[i_p50]
        p50_series = pd.Series(data=p50_data, index=forecast.index)
        p50_series.plot(color="gray", ls="-",marker="s", label=f"{label_prefix}median", ax=ax)

        for i in range(len(percentiles_sorted) // 2):
            ptile = percentiles_sorted[i]
            alpha = alpha_for_percentile(ptile)
            ax.fill_between(
                forecast.index,
                ps_data[i],
                ps_data[-i - 1],
                facecolor=color,
                alpha=alpha,
                interpolate=True,
            )
            # Hack to create labels for the error intervals.
            # Doesn't actually plot anything, because we only pass a single data point
            pd.Series(data=p50_data[:1], index=forecast.index[:1]).plot(
                color=color,
                alpha=alpha,
                linewidth=10,
                label=f"{label_prefix}{100 - ptile * 2}%",
                ax=ax,
            )

    legend = ["observations", "median prediction"] + [f"{k}% prediction interval" for k in prediction_intervals][::-1]
    axx[0].legend(legend, loc="upper left")

    if fname is not None:
        plt.savefig(fname, bbox_inches='tight', pad_inches=0.05)

plot(
    target=targets[0],
    forecast=forecasts[0],
    prediction_length=H,contex_length=M,fname=f"/content/GDDPM_on_renew3_with_full_graph{M}-{H}.jpg", color="limegreen",
)
plt.show()